{
  "Version": "002",
  "Year": "2024",
  "Semester": "Spring",
  "project_name": "Language Detection Using Audio Data",
  "Objective": " \n            The goal of this project is to use deep learning neural networks to develop a language detection program of audio \n            data. Using short audio clips from different languages, accents, and dialects, we believe it will be possible to \n            develop a model that will be able to distinguish the different languages in an efficient and precise manner. \n            ",
  "Dataset": "\n            The dataset that we are using is called the Common Voice Corpus 16.1. This contains thousands of short clips of people \n            speaking in their respective languages on a voluntary basis. It contains over 3,438 hours of audio in 120 different \n            languages. Conveniently, the data is already located in Hugging Face which makes it much more accessible to us.\n\n            The link to the original database can be found here: https://commonvoice.mozilla.org/en/datasets\n\n            And the Hugging Face datasource can be found here: https://huggingface.co/datasets/mozilla-foundation/common_voice_16_1\n\n            ",
  "Rationale": "\n            Language barriers are often a common problem in communication. Whether it be for educational, business, travel, or \n            conversational reasons, not understanding each other's language can make it impossible to communicate effectively. \n            Technology can aid in this communication in various ways. The first step to facilitate effective communication across \n            languages is to identify what language is being spoken. This can be difficult considering different dialects and accents \n            across different languages. Having an effective program that will be able to understand the language through just a \n            short clip can initiate communication between people using different languages.\n            ",
  "Approach": "\n            We will begin this problem by accessing the dataset and doing some EDA. We found that many languages had little audio \n            data, so we decided to limit to languages with over 100 hours of data, which comes down to 43 languages. After the EDA \n            and preparing the data, we will begin with experimenting with different model architectures. Convolutional neural \n            networks are effective with image recognition problems, and we think they would be promising for a project like this \n            one, too. After selecting a model architecture we will be doing model tuning to perfect its performance with language \n            detection.\n            ",
  "Timeline": "\n            Jan 29: Proposal\n            Feb 5: Elevator pitch\n            Feb 12: EDA and research on NN with audio data\n            Feb 19: Preliminary network design\n            Feb 26: Continue with network design\n            March 4: Fine tuning network\n            April 15: Fine tuning network\n            April 22: Fine tuning network\n            April 29: Poster session\n            May 3: Video Presentation\n            May 6: Final report due\n\n            ",
  "Expected Number Students": "\n            There will be two students on this project: Carrie Magee and Jack McMorrow \n            ",
  "Possible Issues": "\n            Considering that the data is provided at a voluntary basis, this could prove to have some issues regarding representation \n            of various language groups. As we have already seen, some languages do not have a lot of samples, which we may decide to \n            leave out of the model. Additionally, the full dataset is very large, at around 80 gigabytes. Therefore, management and \n            storage of the data itself could prove to be a challenge.\n            ",
  "Proposed by": "Jack McMorrow and Carrie Magee",
  "Proposed by email": "jmcmorrow@gwu.edu, mageec@gwu.edu",
  "instructor": "Edwin Lo",
  "instructor_email": "edwinlo@gwu.edu",
  "github_repo": "https://github.com/amir-jafari/Capstone"
}