{
  "Version": "003",
  "Year": "2024",
  "Semester": "Spring",
  "project_name": "Language Detection Using Audio Data",
  "Objective": " \n             The objective of this project is to design a spoken language detection program using human speech recordings provided by\n             the Common Voices dataset. The diverse range of accents, dialects, and languages that make up the dataset will allow us \n             to develop a comprehensive language detection program that can accurately identify and transcribe speech in various languages, \n             regardless of their size or prevalance. We will employ deep learning techniques to train our model and optimize the performance\n             of our language detection system.\"\n            ",
  "Dataset": "\n            As mentioned above, the dataset for our project is called the Common Voice Corpus 16.1 created by Mozilla. \n            The data includes short audio recordings of human speech. The data is collected on a voluntary basis and contains \n            over 3,000 hours of audio recorded in 100+ languages. The audio is stored in MP3 format and \n            accompanied by a corresponding text file containing the spoken content. An updated dataset is released every few months, \n            and we will use the latest version that was released on 1/4/2024. This version of the dataset contains 3,438 recorded hours \n            and 2,856 validated hours, which are the hours of audio that have been validated by other individuals proficient in the \n            language used in the given recording. The Common Voices 16.1 dataset has also been uploaded to the Hugging Face Datasets library \n            which is how we will be accessing the data for our project.   \n\n            Common Voice dataset: https://commonvoice.mozilla.org/en/datasets\n\n            Hugging Face datasource: https://huggingface.co/datasets/mozilla-foundation/common_voice_16_1\n\n            ",
  "Rationale": "\n            In our interconnected world, effective but accessible modes of communication are vital for making lasting connections, \n            spreading information, and fostering understanding. In our capstone class, we have students who speak numerous languages, \n            exhibit various dialects, and possess different accents. Furthermore, as our world relies more and more on virtual \n            communication it's important to address language barriers in virtual spaces. \n            \n            If these virtual communication barriers are not properly addressed they can have real implications in various domains, \n            including in the classroom setting, where students from diverse linguistic backgrounds may face challenges comprehending \n            course materials and actively participating. In the global business world, clear and seamless communication is essential \n            for building strong professional relationships and achieving professional success. Similarly, in healthcare, providers \n            often assist patients who speak different languages or lack the vocabulary required to advocate for their health needs. \n            Language detection systems, which we will be focusing on in our capstone project, can improve things like\n            patient-provider communication. This can lead to more accurate diagnoses and treatment, streamline patients' \n            experiences, and potentially reduce interpretation costs in the medical field. \n            \n            Our capstone project aims to utilize deep learning techniques to create a spoken language detection model. More specifically, \n            our aim is to identify and transcribe human speech in various languages. \n            \n            Success in this project will contribute to effective, accessible communication but also raise cultural awareness to various \n            dialects and languages in our communities. In addition, the outcomes of this project can enhance user interactions in virtual \n            settings and improve their experiences with online content, ultimately contributing to a more immersive digital experience.\n            Most importantly, the results of this project can greatly benefit individuals with hearing-related impairments by improving \n            their ability to interact with spoken language content. In conclusion, by addressing language barriers through projects like ours, \n            we are able to promote inclusivity, accessibility, and interconnectedness within our society.\n            ",
  "Approach": "\n            Our approach will rely on using deep learning techniques, particularly neural networks, to construct our model. More specifically,\n            convolutional neural networks (CNNs), which are often used for image-related data, can be adapted for our language identification system. \n            Through the transformation of audio data into either audio waveforms or spectrograms, visual representations of the audio, we can \n            harness them as inputs for convolutional neural networks. Waveforms are useful for analyzing features such as duration, pauses in speech, and\n            speech rate. Spectograms are useful for collecting information about different phonemes (distinct units of sound) and their frequencies.\n            Another potential model architecture we can explore is transformers. Transformers can be useful for speech recognition (input as audio output as text) or \n            audio classification (input is audio output is probability of specific language). As we delve further into the project, we will gain a clearer \n            understanding of our specific objectives. This will enable us to select the most suitable model architecture for achieving our desired outcomes.\n            ",
  "Timeline": "\n            Jan 29: Proposal\n            Feb 5: Elevator pitch\n            Feb 12: EDA and research on NN with audio data\n            Feb 19: Preliminary network design\n            Feb 26: Continue with network design\n            March 4: Fine tuning network\n            April 15: Fine tuning network\n            April 22: Fine tuning network\n            April 29: Poster session\n            May 3: Video Presentation\n            May 6: Final report due\n\n            ",
  "Expected Number Students": "\n            There will be two students on this project: Carrie Magee and Jack McMorrow \n            ",
  "Possible Issues": "\n            Considering that the data is provided at a voluntary basis, this could prove to have some issues regarding representation \n            of various language groups. Therefore, the program may not perform equally on every individual voice. Also, there is an imbalance\n            between languages in the dataset in terms of recorded hours. Consequently, we will need to determine which languages \n            have sufficient audio data to be effective for training the model. \n            ",
  "Proposed by": "Carrie Magee",
  "Proposed by email": "mageec@gwu.edu",
  "instructor": "Edwin Lo",
  "instructor_email": "edwinlo@gwu.edu",
  "github_repo": "https://github.com/mageeca/24Spr_CMagee_LanguageDetection "
}